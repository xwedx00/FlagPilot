BACKEND ANALYSIS & CREDIT SYSTEM PLAN
=====================================

1. Backend Architecture
-----------------------
- **Frameworks**: FastAPI (Web), MetaGPT (Agents), RAGFlow (Knowledge Base).
- **Service Stack**: 
  - `backend`: logic layer.
  - `postgres`: primary data store.
  - `redis`: caching/queue.
  - `ragflow`: vector store and RAG engine (ElasticSearch + MinIO + MySQL).
- **Execution Flow**:
  - `routers/stream.py`: Handles SSE/WebSocket connections for "Missions" and "Workflows".
  - `agents/team.py`: Orchestrates the Multi-Agent system.
  - `metagpt`: Used as the core agent framework.

2. Database Persistence (PostgreSQL)
------------------------------------
The database schema (`backend/models/`) is comprehensive:
- **Identity**: `User`, `Session` (Managed by Better Auth / Frontend), `Account`.
- **Intelligence**: 
  - `Project`: Container for tasks.
  - `Mission`: Chat sessions (`MissionStatus`, `ChatMessage`).
  - `AgentTask`: Individual units of work (`cost_credits` field exists!).
  - `WorkflowExecution`: DAG execution records.
- **Credits (Already Exists!)**:
  - `CreditWallet`: `user_id`, `balance`.
  - `CreditTransaction`: `wallet_id`, `amount`, `reference_id`.

3. Existing Credit System
-------------------------
A basic credit system is ALREADY implemented in:
- `backend/lib/credits.py`: `CreditService` (add, deduct, check_balance).
- `backend/routers/credits.py`: API endpoints for balance/history.
- `backend/routers/stream.py`: 
  - Charges a **flat fee** of 10 credits per Workflow run.
  - Checks balance before execution.

4. The Gap: Detailed Token Usage
--------------------------------
The current system charges a flat fee per "run". It does NOT charge based on:
- Actual Token Usage (Input/Output).
- Model tier (GPT-4 vs GPT-3.5 costs).
- RAG retrieval costs.

5. Implementation Plan: "Proper LLM Usage Credit System"
--------------------------------------------------------
To implement accurate, usage-based billing:

### A. Token Counting Strategy
We need to capture the `usage` stats from LLM responses.
- **Library**: `tiktoken` (for estimation) is already in requirements.
- **Integration Point**: The `MetaGPT` LLM wrapper. 

### B. Proposed Changes
1. **Create `CostManager`**: A service to calculate cost based on `(model_name, input_tokens, output_tokens)`.
2. **Wrap LLM Calls**:
   - Create a custom `CostAwareLLM` or patch the existing `metagpt.llm.LLM` class.
   - In `backend/agents/team.py`, when an agent returns a result, we must capture the token usage.
   - *Challenge*: MetaGPT abstractions might hide the raw usage stats. We might need to inspect the `Message` object or the raw provider response.
3. **Refine `AgentTask`**:
   - Populate the `cost_credits` field in `AgentTask` with the ACTUAL calculated cost.
4. **Deduct on Completion**:
   - Instead of (or in addition to) the flat fee, deduct the variable cost after the task/step finishes.
   - Update `CreditService.deduct_credits` to handle micro-transactions if needed, or batch them.

### C. Testing
- Results of `run all tests`:
  - 11 Tests Collected.
  - 10 Passed.
  - 1 Failed (`test_stress_global_wisdom.py` - likely RAG timeout or config issue).
  - The failure in stress test implies `Global Wisdom` (RAG) might be flaky, but the core logic is sound.

6. Conclusion
-------------
The foundation is solid. The work is primarily **Integration** involves hooking into the LLM response loop to extract token counts and calling the existing `CreditService`.
